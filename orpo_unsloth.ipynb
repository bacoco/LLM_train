{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bacoco/LLM_train/blob/main/orpo_unsloth.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOImINvpyR0k"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install --no-deps \"xformers<0.0.26\" trl peft accelerate bitsandbytes\n",
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "from google.colab import userdata\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from trl import ORPOConfig, ORPOTrainer\n",
        "from transformers import TextStreamer\n",
        "import pprint"
      ],
      "metadata": {
        "id": "Pu1W0ogRyaJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wb_token = userdata.get('wandb')\n",
        "wandb.login(key=wb_token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSMl9YAc0RhY",
        "outputId": "7859d687-f965-4b3a-c2dc-8a3feced2912"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33meduardovasquez_007\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/llama-3-8b-bnb-4bit\",\n",
        "    max_seq_length=8192,\n",
        "    dtype=None,\n",
        "    load_in_4bit=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIh57LOi0W-v",
        "outputId": "dbdd4c99-88c8-43c0-952b-797ee633a33a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth: Fast Llama patching release 2024.4\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform = Linux.\n",
            "O^O/ \\_/ \\    Pytorch: 2.2.1+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n",
            "\\        /    Bfloat16 = FALSE. Xformers = 0.0.25.post1. FA = False.\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcvCzAFs2HS4",
        "outputId": "b5a1a04d-05ea-4e86-e27d-04013eb1c025"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2024.4 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NlSc8K_w4ttp",
        "outputId": "fa84dc50-8d0e-4639-953d-48d9bb8bdb0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 41,943,040 || all params: 8,072,204,288 || trainable%: 0.5195983464188562\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"reciperesearch/dolphin-sft-v0.1-preference\")[\"train\"]"
      ],
      "metadata": {
        "id": "_tz9Be_h4xBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
        "\n",
        "def format_prompt(sample):\n",
        "    instruction = sample[\"instruction\"]\n",
        "    input       = sample[\"input\"]\n",
        "    accepted    = sample[\"accepted\"]\n",
        "    rejected    = sample[\"rejected\"]\n",
        "\n",
        "    # ORPOTrainer expects prompt/chosen/rejected keys\n",
        "    # See: https://huggingface.co/docs/trl/main/en/orpo_trainer\n",
        "    sample[\"prompt\"]   = alpaca_prompt.format(instruction, input, \"\")\n",
        "    sample[\"chosen\"]   = accepted + EOS_TOKEN\n",
        "    sample[\"rejected\"] = rejected + EOS_TOKEN\n",
        "    return sample\n",
        "pass"
      ],
      "metadata": {
        "id": "diUQpluu45w5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.map(format_prompt)"
      ],
      "metadata": {
        "id": "ju84Qz0G8DzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "row = dataset[1]\n",
        "print('INSTRUCTION: ' + '=' * 50)\n",
        "pprint.pprint(row[\"prompt\"])\n",
        "print('ACCEPTED: ' + '=' * 50)\n",
        "pprint.pprint(row[\"chosen\"])\n",
        "print('REJECTED: ' + '=' * 50)\n",
        "pprint.pprint(row[\"rejected\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRM-yG558IJu",
        "outputId": "45051c2e-b903-42f1-cb59-2adb72ff3282"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INSTRUCTION: ==================================================\n",
            "('Below is an instruction that describes a task, paired with an input that '\n",
            " 'provides further context. Write a response that appropriately completes the '\n",
            " 'request.\\n'\n",
            " '\\n'\n",
            " '### Instruction:\\n'\n",
            " 'You are an AI assistant that helps people find information.\\n'\n",
            " '\\n'\n",
            " '### Input:\\n'\n",
            " 'Given the rationale, provide a reasonable question and answer. Step-by-step '\n",
            " 'reasoning process: Xkcd comics are very popular amongst internet users.\\n'\n",
            " ' The question and answer:\\n'\n",
            " '\\n'\n",
            " '### Response:\\n')\n",
            "ACCEPTED: ==================================================\n",
            "('Question: What makes Xkcd comics popular among internet users?\\n'\n",
            " '\\n'\n",
            " 'Answer: Xkcd comics are popular among internet users because of their clever '\n",
            " 'humor, relatable themes, and minimalist art style. They often cover topics '\n",
            " 'like science, technology, and life experiences, making them appealing to a '\n",
            " 'broad audience.<|end_of_text|>')\n",
            "REJECTED: ==================================================\n",
            "('Question: What is the reason behind the popularity of Xkcd comics among '\n",
            " 'internet users?\\n'\n",
            " '\\n'\n",
            " 'Answer: Xkcd comics are popular among internet users because they offer a '\n",
            " 'unique blend of humor, relatable content, and thought-provoking topics that '\n",
            " 'resonate with a wide range of people. The comics often address everyday '\n",
            " 'experiences, technology, and social issues, making them accessible and '\n",
            " 'enjoyable for many individuals. Additionally, the simple and minimalistic '\n",
            " 'art style of Xkcd comics allows for easy comprehension and sharing, '\n",
            " 'contributing to their widespread appeal.<|end_of_text|>')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = dataset.shuffle(seed=10).select(range(3000))\n",
        "eval_dataset = dataset.shuffle(seed=10).select(range(1000))"
      ],
      "metadata": {
        "id": "yEj5PJa1-B9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "orpo_args = ORPOConfig(\n",
        "        max_length = 8192,\n",
        "        max_prompt_length = 8192//2,\n",
        "        max_completion_length = 8192//2,\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        beta = 0.1, # optimization process\n",
        "        logging_steps = 1, #frequency of logging metrics\n",
        "        optim = \"adamw_8bit\",\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        max_steps = 10, # Change to num_train_epochs = 1 for full training runs\n",
        "        fp16 = not torch.cuda.is_bf16_supported(), # Whether to use 16-bit floating-point precision for training\n",
        "        bf16 = torch.cuda.is_bf16_supported(), # Whether to use 16-bit bfloat16 precision for training\n",
        "        output_dir = \"outputs\",\n",
        "        report_to=\"wandb\",\n",
        "    )"
      ],
      "metadata": {
        "id": "wZWTUZv8_P0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "orpo_trainer = ORPOTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    args=orpo_args\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "061EGzcL_b54",
        "outputId": "754d29b9-2c47-4935-d0e7-9a4738afdb06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/orpo_trainer.py:247: UserWarning: When using DPODataCollatorWithPadding, you should set `remove_unused_columns=False` in your TrainingArguments we have set it for you, but you should do it yourself in the future.\n",
            "  warnings.warn(\n",
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "orpo_trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 614
        },
        "id": "Yx-_exAC_pHJ",
        "outputId": "9b8f95d1-3610-4485-9db6-91aac5927e1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
            "   \\\\   /|    Num examples = 3,000 | Num Epochs = 1\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
            "\\        /    Total batch size = 8 | Total steps = 10\n",
            " \"-____-\"     Number of trainable parameters = 41,943,040\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.16.6"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240504_095514-or91z102</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/eduardovasquez_007/huggingface/runs/or91z102' target=\"_blank\">light-carrier-15</a></strong> to <a href='https://wandb.ai/eduardovasquez_007/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/eduardovasquez_007/huggingface' target=\"_blank\">https://wandb.ai/eduardovasquez_007/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/eduardovasquez_007/huggingface/runs/or91z102' target=\"_blank\">https://wandb.ai/eduardovasquez_007/huggingface/runs/or91z102</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10/10 02:52, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>4.379300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>4.794500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>5.025300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.866000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.922000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>2.279200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.945300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.978900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.965300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.655200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=10, training_loss=2.8810996890068052, metrics={'train_runtime': 204.7093, 'train_samples_per_second': 0.391, 'train_steps_per_second': 0.049, 'total_flos': 0.0, 'train_loss': 2.8810996890068052, 'epoch': 0.02666666666666667})"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# alpaca_prompt = Copied from above\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"You are an AI assistant. User will you give you a task. Your goal is to complete the task as faithfully as you can. While performing the task think step-by-step and justify your steps.\", # instruction\n",
        "        \"Solve the following algebraic equation: 2x + 5 = 15\", # input\n",
        "        \"\", # output - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5Dps50EB3u7",
        "outputId": "c8343986-6227-4105-9845-42a38af1a033"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "You are an AI assistant. User will you give you a task. Your goal is to complete the task as faithfully as you can. While performing the task think step-by-step and justify your steps.\n",
            "\n",
            "### Input:\n",
            "Solve the following algebraic equation: 2x + 5 = 15\n",
            "\n",
            "### Response:\n",
            "2x + 5 = 15\n",
            "2x = 10\n",
            "x = 5<|end_of_text|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"You are an AI assistant. User will you give you a task. Your goal is to complete the task as faithfully as you can. While performing the task think step-by-step and justify your steps.\", # instruction\n",
        "        \"Describe the steps to take when preparing for a job interview.\", # input\n",
        "        \"\", # output - leave this blank for generation!\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZSHGZu3DYOx",
        "outputId": "efdb2bb7-87ad-4e27-c0f3-82845bff8065"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|begin_of_text|>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "You are an AI assistant. User will you give you a task. Your goal is to complete the task as faithfully as you can. While performing the task think step-by-step and justify your steps.\n",
            "\n",
            "### Input:\n",
            "Describe the steps to take when preparing for a job interview.\n",
            "\n",
            "### Response:\n",
            "1. Research the company and the position you are applying for.\n",
            "2. Prepare a list of questions to ask during the interview.\n",
            "3. Practice answering common interview questions.\n",
            "4. Dress professionally and arrive early.\n",
            "5. Be confident and enthusiastic.\n",
            "6. Follow up after the interview with a thank-you note.<|end_of_text|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"lora_model\")\n",
        "tokenizer.save_pretrained(\"lora_model\")"
      ],
      "metadata": {
        "id": "d1s_D_cFEX7C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}