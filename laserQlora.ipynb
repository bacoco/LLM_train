{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "qJhqNXDPPAuC",
        "mgIqJzUpJUoA",
        "Df2bWWgCOFHm"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bacoco/LLM_train/blob/main/laserQlora.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **laser-QLoRA - A new way to train your model for a specific task.**\n",
        "\n",
        "by [Fernando Fernandes Neto](https://twitter.com/FernandoNetoAi), [David Golchinfar](https://twitter.com/DavidGFar) and [Eric Hartford](https://twitter.com/erhartford)\n",
        "\n",
        "supported by [VAGO solutions](https://vago-solutions.de) and [Hyperspace.ai](https://hyperspace.computer)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "With this notebook, we present a novel training strategy for the SFT and DPO training process, in which we partially freeze the model after a laser-like analysis to navigate and optimize the trade-offs highlighted by the no-free-lunch theorem. This innovative training method effectively prevents the major problem of language models forgetting previously acquired knowledge. This aspect is particularly important when trying to teach the model specific skills, such as a new language, where the model could generally lose a significant amount of its prior knowledge and show a decline in overall intelligence.\n",
        "\n",
        "The main contribution of the following script facilitates the discovery of layers possessing superior signal-to-noise efficiency, signifying those that might be more impactful or essential for the model's effectiveness. Layers exhibiting higher SNR ratios in comparison to their maximum singular value are viewed as having weights that more significantly enhance the model's output, laying the groundwork for optimizing and refining the model.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fVVMA_XfGDAM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Overview:**\n",
        "\n",
        "Here, we provide an exemplary demonstration of what training with laser-QLoRA might look like. To give you a brief overview of the process: Initially, the script is executed, which, among other outputs, generates a JSON file containing the current top 16 highest SNR/max singular value for each module in every layer. Following this, we will guide you on how to use the extracted layers in Axolotl or LlamaFactory for your training.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "W5lajSuSO27A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **The laser-scanner script:**"
      ],
      "metadata": {
        "id": "qJhqNXDPPAuC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9aZQkBb0FpPc"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# %%\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import numpy as np\n",
        "import gc\n",
        "import json\n",
        "\n",
        "\n",
        "# %%\n",
        "model_name = \"mistralai/Mistral-7B-v0.1\"  # Change to your preferred model\n",
        "\n",
        "class ModelModifier:\n",
        "    def __init__(self, model_name):\n",
        "        self.model_name = model_name\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map={\"\":0})\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.layer_snr = {}\n",
        "        self.modified_layers = set()\n",
        "        self.original_weights = {}\n",
        "\n",
        "    def calculate_snr_for_layer(self, layer_type, layer_number):\n",
        "        for name, module in self.model.named_modules():\n",
        "            if layer_type in name and str(layer_number) in name:\n",
        "                weights = module.weight.double()\n",
        "                S = torch.linalg.svdvals(weights)\n",
        "                max_singular_value = S[0].item()  # First singularity value\n",
        "                weights = weights.detach().cpu()\n",
        "                S = S.detach().cpu()\n",
        "                sigma_estimated = self.estimate_sigma_with_full_iqr(S)\n",
        "                n, m = weights.shape\n",
        "                mp_threshold = self.marchenko_pastur_threshold(sigma_estimated, n, m)\n",
        "\n",
        "                signal = S[S > mp_threshold].sum()\n",
        "                noise = S[S <= mp_threshold].sum()\n",
        "                snr = signal / noise if noise != 0 else float('inf')\n",
        "                snr_ratio = snr / max_singular_value  # Calculates the ratio of SNR to the highest singularity value\n",
        "                del S, weights\n",
        "                torch.cuda.empty_cache()  # Clear PyTorch's CUDA memory cache\n",
        "                gc.collect()\n",
        "                return snr_ratio  # Returns the ratio\n",
        "    @staticmethod\n",
        "    def marchenko_pastur_threshold(sigma, n, m):\n",
        "        beta = n / m if n < m else m / n\n",
        "        threshold = sigma * np.sqrt((1 + np.sqrt(beta))**2)\n",
        "        return threshold\n",
        "\n",
        "    ## Calculate an estimate of the standard deviation of the singular values based on Inter Quantile Range\n",
        "\n",
        "    @staticmethod\n",
        "    def estimate_sigma_with_full_iqr(S):\n",
        "        q75 = torch.quantile(S, 0.75)\n",
        "        q25 = torch.quantile(S, 0.25)\n",
        "        iqr = q75 - q25\n",
        "        sigma_estimated = iqr / 1.349 ## 0.6745 * sigma is the expected range between the quantiles (Q1 and Q3)\n",
        "        return sigma_estimated\n",
        "\n",
        "\n",
        "    def assess_layers_snr(self, layer_types, layer_numbers):\n",
        "        for name, module in self.model.named_modules():\n",
        "            for layer_number in layer_numbers:\n",
        "                for layer_type in layer_types:\n",
        "                    if layer_type in name and str(layer_number) in name:\n",
        "                        print(\"*\"*50, flush=True)\n",
        "                        print(f\"Calculating Signal to Noise Ratio at layer {name}\", flush=True)\n",
        "                        snr_ratio = self.calculate_snr_for_layer(layer_type, layer_number)\n",
        "                        self.layer_snr[name] = {'snr_ratio': snr_ratio, 'module': name}\n",
        "                        print(f\"Signal to Noise Ratio at layer {name} = {snr_ratio}\", flush=True)\n",
        "                        print(\"*\"*50, flush=True)\n",
        "\n",
        "\n",
        "    def save_layers_to_json(self, filename=\"layer_snr_info.json\"):\n",
        "        with open(filename, 'w') as file:\n",
        "            serializable_data = {}\n",
        "            for key, value in self.layer_snr.items():\n",
        "                # Convert Tensors to Python numbers (for SNR) and handle other data types as needed\n",
        "                snr_value = value['snr_ratio'].item() if isinstance(value['snr_ratio'], torch.Tensor) else value['snr_ratio']\n",
        "                module_str = str(value['module'])  # Assuming module representation is a string or convertible to a string\n",
        "                serializable_data[key] = {'snr': snr_value, 'module': module_str}\n",
        "\n",
        "            json.dump(serializable_data, file, indent=4)\n",
        "\n",
        "\n",
        "\n",
        "    def get_top_snr_ratios(self, top_n=16):\n",
        "        # Initialize a dictionary to store the SNR ratios for the specific modules\n",
        "        snr_ratios_per_specific_module = {\n",
        "            'self_attn.v_proj': [],\n",
        "            'self_attn.k_proj': [],\n",
        "            'self_attn.o_proj': [],\n",
        "            'self_attn.q_proj': [],\n",
        "            'mlp.down_proj': [],\n",
        "            'mlp.up_proj': [],\n",
        "            'mlp.gate_proj': []\n",
        "        }\n",
        "\n",
        "        # Run through all layer SNR entries\n",
        "        for name, value in self.layer_snr.items():\n",
        "            snr_ratio = value['snr_ratio']\n",
        "            layer_name = value['module']\n",
        "\n",
        "            # For each specific module, check if the layer name contains the module\n",
        "            for specific_module in snr_ratios_per_specific_module.keys():\n",
        "                if specific_module in layer_name:\n",
        "                    # Add the layer name and SNR value to the corresponding entry\n",
        "                    snr_ratios_per_specific_module[specific_module].append((layer_name, snr_ratio))\n",
        "                    break  # End the loop when the module is found to avoid duplicate entries\n",
        "\n",
        "        # Sort and extract the top 16 SNR values for each specific module\n",
        "        top_snr_layers = {}\n",
        "        for module, snr_ratios in snr_ratios_per_specific_module.items():\n",
        "            sorted_layers = sorted(snr_ratios, key=lambda x: x[1], reverse=True)  # Sort by SNR value\n",
        "            top_snr_layers[module] = [layer[0] for layer in sorted_layers[:top_n]]  # Saving the layer names\n",
        "\n",
        "        return top_snr_layers\n",
        "\n",
        "\n",
        "    def save_top_snr_ratios_to_json(self, top_snr_layers, filename=\"top_snr_ratios.json\"):\n",
        "        with open(filename, 'w') as file:\n",
        "            json.dump(top_snr_layers, file, indent=4)\n",
        "\n",
        "\n",
        "# Usage\n",
        "modifier = ModelModifier(model_name)\n",
        "\n",
        "# %%\n",
        "layer_numbers = list(range(31, -1, -1))\n",
        "layer_numbers = [f\".{l}.\" for l in layer_numbers]\n",
        "print(layer_numbers)\n",
        "\n",
        "layer_types=['mlp.gate', 'mlp.down_proj', 'mlp.up_proj', 'self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj']\n",
        "\n",
        "# %%\n",
        "## Search all layers and get the SNR/max singularity value\n",
        "\n",
        "modifier.assess_layers_snr(layer_types, layer_numbers)\n",
        "top_snr_ratios = modifier.get_top_snr_ratios() # Define your specific top_n here otherwise it will be top_n=16\n",
        "\n",
        "print(\"Finished laserRMT scanning.\", flush=True)\n",
        "\n",
        "# Save the layer information to a JSON file\n",
        "modifier.save_top_snr_ratios_to_json(\"laser_scan_mistral_top_snr.json\")\n",
        "modifier.save_layers_to_json(\"laser_scan_mistral.json\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The key part of this script is the **calculate_snr_for_layer** function:\n",
        "The `calculate_snr_for_layer` method in the Python code performs a detailed analysis of the signal-to-noise ratio (SNR) for a specific layer within a neural network model. This method incorporates both the extraction of singular values from the layer's weights and the application of statistical measures to determine the layer's SNR. Here's a step-by-step breakdown of the process, integrating the mathematical concepts and formulas addressed previously:\n",
        "\n",
        "1. **Identify Layer Weights**: For a given layer type and number, the method iterates through the model's layers to find a match. Once found, it extracts the weights of the layer and converts them to double precision for accurate computation.\n",
        "\n",
        "2. **Singular Value Decomposition (SVD) Values**: The method calculates the singular values (\\(S\\)) of the layer's weight matrix using PyTorch's `torch.linalg.svdvals` function. This step is crucial for assessing the layer's information content through its singular values.\n",
        "\n",
        "3. **Maximum Singular Value**: It records the maximum singular value (\\(S[0]\\)), which represents the highest magnitude of signal strength in the layer's weights.\n",
        "\n",
        "4. **Estimate Sigma with IQR**: Using the full inter-quantile range (IQR) method, it estimates the standard deviation (\\(\\sigma\\)) of the singular values. This estimation helps in setting a threshold for distinguishing between signal and noise based on the variability of the singular values:\n",
        "   \\[\\sigma = \\frac{IQR}{1.349}\\]\\\n",
        "\n",
        "\n",
        "5. **Marchenko-Pastur Threshold**: The method then calculates the Marchenko-Pastur threshold (\\(\\lambda\\)) to separate the singular values into signal and noise categories. This threshold is computed using the formula:\n",
        "   \\[\\lambda = \\sigma \\sqrt{(1 + \\sqrt{\\beta})^2}\\]\n",
        "   where \\(\\beta\\) is the aspect ratio of the weight matrix (\\(n/m\\) or \\(m/n\\), whichever is smaller).\n",
        "\n",
        "6. **Signal and Noise Calculation**: The singular values greater than the Marchenko-Pastur threshold (\\(\\lambda\\)) are considered signal, and those below are considered noise. The method sums these groups of singular values separately to quantify the total signal (\\(\\sum_{\\sigma_i > \\lambda} \\sigma_i\\)) and total noise (\\(\\sum_{\\sigma_i \\leq \\lambda} \\sigma_i\\)).\n",
        "\n",
        "7. **Signal-to-Noise Ratio (SNR)**: The SNR is calculated by dividing the total signal by the total noise. In cases where the noise is zero (to avoid division by zero), the SNR is set to infinity (\\(\\infty\\)), indicating a layer with overwhelmingly dominant signal content.\n",
        "\n",
        "8. **SNR Ratio Relative to Maximum Singular Value**: The method further refines the SNR analysis by calculating the ratio of the SNR to the maximum singular value. This ratio provides insight into how the layer's strongest signal component compares to the overall signal-to-noise balance:\n",
        "   \\[SNR\\ Ratio = \\frac{SNR}{\\text{max singular value}}\\]\n",
        "\n",
        "9. **Memory Management**: After the calculations, the method clears the allocated memory for the singular values and weights to optimize memory usage and prevent memory leaks.\n",
        "\n",
        "This detailed analysis enables the identification of layers with high signal-to-noise efficiency, indicating layers that are potentially more influential or critical to the model's performance. Layers with higher SNR ratios relative to their maximum singular value are considered to have weights that are more effectively contributing to the model's output, providing a basis for model optimization and refinement.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "A6vOzpS7KgV7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Next step after the script is finished:**\n",
        "\n",
        "\n",
        "First , we will concentrate only on the content of laser_scan_mistral_top_snr.json.\n",
        "The result will something like this:\n",
        "\n"
      ],
      "metadata": {
        "id": "mgIqJzUpJUoA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Example content of laser_scan_mistral_top_snr.json:\n",
        "\n",
        "```\n",
        "{\n",
        "    \"self_attn.v_proj\": [\n",
        "        \"model.layers.3.self_attn.v_proj\",\n",
        "        \"model.layers.2.self_attn.v_proj\",\n",
        "        \"model.layers.1.self_attn.v_proj\",\n",
        "        \"model.layers.0.self_attn.v_proj\",\n",
        "\t...,\n",
        "\t..\n",
        "    ],\n",
        "    \"self_attn.k_proj\": [\n",
        "        \"model.layers.2.self_attn.k_proj\",\n",
        "        \"model.layers.0.self_attn.k_proj\",\n",
        "        \"model.layers.1.self_attn.k_proj\",\n",
        "        \"model.layers.3.self_attn.k_proj\",\n",
        "\t...,\n",
        "\t..\n",
        "    ],\n",
        "    \"self_attn.o_proj\": [\n",
        "        \"model.layers.0.self_attn.o_proj\",\n",
        "        \"model.layers.3.self_attn.o_proj\",\n",
        "        \"model.layers.2.self_attn.o_proj\",\n",
        "        \"model.layers.1.self_attn.o_proj\",\n",
        "\t...,\n",
        "\t..\n",
        "    ],\n",
        "    \"self_attn.q_proj\": [\n",
        "        \"model.layers.0.self_attn.q_proj\",\n",
        "        \"model.layers.1.self_attn.q_proj\",\n",
        "        \"model.layers.2.self_attn.q_proj\",\n",
        "        \"model.layers.3.self_attn.q_proj\",\n",
        "\t...,\n",
        "\t..\n",
        "    ],\n",
        "    \"mlp.down_proj\": [\n",
        "        \"model.layers.1.mlp.down_proj\",\n",
        "        \"model.layers.2.mlp.down_proj\",\n",
        "        \"model.layers.3.mlp.down_proj\",\n",
        "        \"model.layers.0.mlp.down_proj\",\n",
        "\t...,\n",
        "\t..\n",
        "    ],\n",
        "    \"mlp.up_proj\": [\n",
        "        \"model.layers.3.mlp.up_proj\",\n",
        "        \"model.layers.2.mlp.up_proj\",\n",
        "        \"model.layers.1.mlp.up_proj\",\n",
        "        \"model.layers.0.mlp.up_proj\",\n",
        "\t...,\n",
        "\t..\n",
        "    ],\n",
        "    \"mlp.gate_proj\": [\n",
        "        \"model.layers.3.mlp.gate_proj\",\n",
        "        \"model.layers.2.mlp.gate_proj\",\n",
        "        \"model.layers.1.mlp.gate_proj\",\n",
        "        \"model.layers.0.mlp.gate_proj\",\n",
        "\t...,\n",
        "\t..\n",
        "    ]\n",
        "}\n",
        "```\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3UUBWnKHJCvz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **The procedure for Axolotl:**"
      ],
      "metadata": {
        "id": "Df2bWWgCOFHm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Go to your axolotl config.yml:\n",
        "in this example we used the top 16 snr values of the dolphin-2.6-mistral-7b-dpo-laser\n",
        "\n",
        "\n",
        "```\n",
        "base_model: cognitivecomputations/dolphin-2.6-mistral-7b-dpo-laser\n",
        "model_type: MistralForCausalLM\n",
        "tokenizer_type: LlamaTokenizer\n",
        "is_mistral_derived_model: true\n",
        "\n",
        "load_in_8bit: false\n",
        "load_in_4bit: true\n",
        "strict: false\n",
        "\n",
        "datasets:\n",
        "#we used a small dataset to teach the model function calling abilities\n",
        "  - path: ./data/function_calling_2k.json\n",
        "    ds_type: json\n",
        "    type: sharegpt\n",
        "\n",
        "dataset_prepared_path: last_run_function_call\n",
        "#0.05\n",
        "val_set_size: 0.02\n",
        "output_dir: ./laser-qlora-out-dolphin-function-top16\n",
        "\n",
        "adapter: qlora\n",
        "lora_model_dir:\n",
        "\n",
        "sequence_len: 8192\n",
        "sample_packing: false\n",
        "eval_sample_packing: true\n",
        "pad_to_sequence_len: true\n",
        "\n",
        "# important, to get the same trainable parameters then for a qlora training with lora_r=32 and lora_alpha=16 you need to adjust the lora_r depending on the amount of filtered layers you want to use. With top_n=4 you would go for lora_r= 256\n",
        "\n",
        "lora_r: 64\n",
        "lora_alpha: 16\n",
        "lora_dropout: 0.05\n",
        "lora_target_linear: false\n",
        "lora_fan_in_fan_out:\n",
        "lora_target_modules:\n",
        "  - layers.30.self_attn.q_proj\n",
        "  - layers.0.self_attn.q_proj\n",
        "  - layers.1.self_attn.q_proj\n",
        "  - layers.15.self_attn.q_proj\n",
        "  - layers.12.self_attn.q_proj\n",
        "  - layers.11.self_attn.q_proj\n",
        "  - layers.14.self_attn.q_proj\n",
        "  - layers.9.self_attn.q_proj\n",
        "  - layers.16.self_attn.q_proj\n",
        "  - layers.18.self_attn.q_proj\n",
        "  - layers.13.self_attn.q_proj\n",
        "  - layers.10.self_attn.q_proj\n",
        "  - layers.7.self_attn.q_proj\n",
        "  - layers.8.self_attn.q_proj\n",
        "  - layers.4.self_attn.q_proj\n",
        "  - layers.19.self_attn.q_proj\n",
        "  - layers.27.self_attn.k_proj\n",
        "  - layers.24.self_attn.k_proj\n",
        "  - layers.25.self_attn.k_proj\n",
        "  - layers.22.self_attn.k_proj\n",
        "  - layers.26.self_attn.k_proj\n",
        "  - layers.29.self_attn.k_proj\n",
        "  - layers.23.self_attn.k_proj\n",
        "  - layers.28.self_attn.k_proj\n",
        "  - layers.21.self_attn.k_proj\n",
        "  - layers.31.self_attn.k_proj\n",
        "  - layers.30.self_attn.k_proj\n",
        "  - layers.20.self_attn.k_proj\n",
        "  - layers.5.self_attn.k_proj\n",
        "  - layers.19.self_attn.k_proj\n",
        "  - layers.17.self_attn.k_proj\n",
        "  - layers.18.self_attn.k_proj\n",
        "  - layers.31.self_attn.v_proj\n",
        "  - layers.19.self_attn.v_proj\n",
        "  - layers.24.self_attn.v_proj\n",
        "  - layers.18.self_attn.v_proj\n",
        "  - layers.5.self_attn.v_proj\n",
        "  - layers.3.self_attn.v_proj\n",
        "  - layers.16.self_attn.v_proj\n",
        "  - layers.23.self_attn.v_proj\n",
        "  - layers.27.self_attn.v_proj\n",
        "  - layers.25.self_attn.v_proj\n",
        "  - layers.26.self_attn.v_proj\n",
        "  - layers.20.self_attn.v_proj\n",
        "  - layers.6.self_attn.v_proj\n",
        "  - layers.15.self_attn.v_proj\n",
        "  - layers.17.self_attn.v_proj\n",
        "  - layers.29.self_attn.v_proj\n",
        "  - layers.30.self_attn.o_proj\n",
        "  - layers.12.self_attn.o_proj\n",
        "  - layers.9.self_attn.o_proj\n",
        "  - layers.14.self_attn.o_proj\n",
        "  - layers.0.self_attn.o_proj\n",
        "  - layers.6.self_attn.o_proj\n",
        "  - layers.8.self_attn.o_proj\n",
        "  - layers.10.self_attn.o_proj\n",
        "  - layers.11.self_attn.o_proj\n",
        "  - layers.13.self_attn.o_proj\n",
        "  - layers.24.self_attn.o_proj\n",
        "  - layers.5.self_attn.o_proj\n",
        "  - layers.15.self_attn.o_proj\n",
        "  - layers.7.self_attn.o_proj\n",
        "  - layers.17.self_attn.o_proj\n",
        "  - layers.25.self_attn.o_proj\n",
        "  - layers.31.mlp.gate_proj\n",
        "  - layers.30.mlp.gate_proj\n",
        "  - layers.4.mlp.gate_proj\n",
        "  - layers.3.mlp.gate_proj\n",
        "  - layers.28.mlp.gate_proj\n",
        "  - layers.29.mlp.gate_proj\n",
        "  - layers.6.mlp.gate_proj\n",
        "  - layers.27.mlp.gate_proj\n",
        "  - layers.5.mlp.gate_proj\n",
        "  - layers.26.mlp.gate_proj\n",
        "  - layers.25.mlp.gate_proj\n",
        "  - layers.7.mlp.gate_proj\n",
        "  - layers.2.mlp.gate_proj\n",
        "  - layers.24.mlp.gate_proj\n",
        "  - layers.23.mlp.gate_proj\n",
        "  - layers.10.mlp.gate_proj\n",
        "  - layers.30.mlp.up_proj\n",
        "  - layers.4.mlp.up_proj\n",
        "  - layers.6.mlp.up_proj\n",
        "  - layers.5.mlp.up_proj\n",
        "  - layers.27.mlp.up_proj\n",
        "  - layers.25.mlp.up_proj\n",
        "  - layers.26.mlp.up_proj\n",
        "  - layers.17.mlp.up_proj\n",
        "  - layers.24.mlp.up_proj\n",
        "  - layers.7.mlp.up_proj\n",
        "  - layers.10.mlp.up_proj\n",
        "  - layers.3.mlp.up_proj\n",
        "  - layers.23.mlp.up_proj\n",
        "  - layers.11.mlp.up_proj\n",
        "  - layers.9.mlp.up_proj\n",
        "  - layers.14.mlp.up_proj\n",
        "  - layers.29.mlp.down_proj\n",
        "  - layers.19.mlp.down_proj\n",
        "  - layers.20.mlp.down_proj\n",
        "  - layers.18.mlp.down_proj\n",
        "  - layers.21.mlp.down_proj\n",
        "  - layers.1.mlp.down_proj\n",
        "  - layers.28.mlp.down_proj\n",
        "  - layers.22.mlp.down_proj\n",
        "  - layers.23.mlp.down_proj\n",
        "  - layers.30.mlp.down_proj\n",
        "  - layers.4.mlp.down_proj\n",
        "  - layers.17.mlp.down_proj\n",
        "  - layers.2.mlp.down_proj\n",
        "  - layers.15.mlp.down_proj\n",
        "  - layers.27.mlp.down_proj\n",
        "  - layers.5.mlp.down_proj\n",
        "  # important: you need to unfreeze the lm.head\n",
        "  - lm.head\n",
        "\n",
        "wandb_project: axolotl\n",
        "wandb_entity:\n",
        "wandb_watch:\n",
        "wandb_run_id:\n",
        "wandb_log_model:\n",
        "\n",
        "gradient_accumulation_steps: 2\n",
        "micro_batch_size: 2\n",
        "num_epochs: 3\n",
        "optimizer: adamw_bnb_8bit\n",
        "lr_scheduler: cosine\n",
        "learning_rate: 0.00025\n",
        "\n",
        "train_on_inputs: false\n",
        "group_by_length: false\n",
        "bf16: true\n",
        "fp16: false\n",
        "tf32: false\n",
        "\n",
        "gradient_checkpointing: true\n",
        "early_stopping_patience:\n",
        "resume_from_checkpoint:\n",
        "local_rank:\n",
        "logging_steps: 1\n",
        "xformers_attention:\n",
        "flash_attention: true\n",
        "\n",
        "warmup_steps: 100\n",
        "eval_steps: 0.2\n",
        "eval_table_size:\n",
        "eval_table_max_new_tokens: 128\n",
        "save_steps:\n",
        "debug:\n",
        "deepspeed:\n",
        "weight_decay: 0.0\n",
        "fsdp:\n",
        "fsdp_config:\n",
        "special_tokens:\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "Wt6GICOnP2r_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After adjusting your config.yml you can start your axolotl training as usual.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "YNQPhbo8RbWG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **The procedure for Llama-Factory:**"
      ],
      "metadata": {
        "id": "FhB7WE21Rozc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example for a multi-gpu DPO training:\n",
        "\n",
        "\n",
        "```\n",
        "accelerate launch src/train_bash.py \\\n",
        "    --deepspeed dsconfig.json \\\n",
        "    --stage dpo \\\n",
        "    --model_name_or_path your_model \\\n",
        "    --quantization_bit 4 \\\n",
        "    --do_train \\\n",
        "    --dataset your_dataset\\\n",
        "    --template mistral \\\n",
        "    --finetuning_type lora \\\n",
        "    # here you need to add your layers to unfreeze - don't forget lm.head at the end\n",
        "    --lora_target layers.0.self_attn.q_proj,layers.1.self_attn.q_proj,layers.15.self_attn.q_proj,layers.12.self_attn.q_proj,layers.27.self_attn.k_proj,layers.24.self_attn.k_proj,layers.25.self_attn.k_proj,layers.22.self_attn.k_proj,layers.19.self_attn.v_proj,layers.24.self_attn.v_proj,layers.18.self_attn.v_proj,layers.30.self_attn.v_proj,layers.30.self_attn.o_proj,layers.12.self_attn.o_proj,layers.9.self_attn.o_proj,layers.14.self_attn.o_proj,layers.31.mlp.gate_proj,layers.30.mlp.gate_proj,layers.4.mlp.gate_proj,layers.3.mlp.gate_proj,layers.29.mlp.up_proj,layers.6.mlp.up_proj,layers.4.mlp.up_proj,layers.5.mlp.up_proj,layers.30.mlp.down_proj,layers.19.mlp.down_proj,layers.20.mlp.down_proj,layers.18.mlp.down_proj,lm.head \\\n",
        "    --output_dir spin_e1-\\\n",
        "    --per_device_train_batch_size 1 \\\n",
        "    --gradient_accumulation_steps 1 \\\n",
        "    --lr_scheduler_type cosine \\\n",
        "    --logging_steps 1 \\\n",
        "    --save_steps 1000 \\\n",
        "    --learning_rate 2.5e-04 \\\n",
        "    --num_train_epochs 1.0 \\\n",
        "    --plot_loss \\\n",
        "    --bf16 \\\n",
        "    --split train \\\n",
        "    --report_to=wandb \\\n",
        "    --cutoff_len 2000 \\\n",
        "    --save_safetensors \\\n",
        "    --warmup_steps 100 \\\n",
        "    --optim paged_adamw_8bit \\\n",
        "    --lora_dropout 0.05\n",
        "```\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "trv8dLjRRscT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Results**\n",
        "\n",
        "The choice of how many layers you unfreeze is not proportional. Here is an example using the benchmarks from the FC-SauerkrautLM, which we trained with the function_calling dataset:\n",
        "\n",
        "\n",
        "![fc_results.PNG](https://vago-solutions.de/wp-content/uploads/2024/02/fc_results.png)\n",
        "\n",
        "\n",
        "\n",
        "All models are based on the SauerkrautLM-7b-LaserChat model and were trained using the same effective batch size, as well as identical hyperparameters and trainable parameters (adjusted by the value of lora_r). It is observable that, **on average, the laser-QLoRA trainings performed better than classic QLoRA.**\n",
        "\n",
        "In particular, **laser-QLoRA top 3 and laser-QLoRA top 16 significantly outperformed classic QLoRA.** Function calling, as well as other tasks, such as learning new languages or RAG data, with very specific training data, benefit from the laser-QLoRA approach.\n",
        "\n",
        "Our findings indicate that our approach not only surpasses QLoRA in direct training comparisons within the Open LLM leaderboard benchmarks but also reveals that using just a fraction of the typical function calling training data (2,000 samples), a model can effectively identify and employ functions in multi-turn conversations. Traditionally, function calling datasets encompass over 100,000 samples, a scale that could significantly induce the base model to lose a considerable portion of its pre-existing skills during QLoRA training.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xQhmbvr-Wvwr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Future Work**\n",
        "\n",
        "Many more experiments need to be conducted. However, so far, we have been able to identify significant improvements compared to traditional QLoRA training, especially when addressing specific use cases. For instance, companies can significantly enhance their RAG systems through this approach, as their base models can be improved through training with laser-QLoRA. What does this mean precisely? It means that small datasets, which represent the RAG operations can be trained, thereby significantly enhancing their language model's extraction and reasoning capabilities.\n",
        "\n",
        "This approach is not limited to the use in large language models. Visual models, such as stable diffusion, can also be significantly optimized through the laser scanner and subsequent laser-QLoRA training."
      ],
      "metadata": {
        "id": "sFPK5g6EsTkV"
      }
    }
  ]
}