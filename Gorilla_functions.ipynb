{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bacoco/LLM_train/blob/main/Gorilla_functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OpenFunctions from Gorilla - Try it out in less than 60s üöÄ\n",
        "\n",
        "[![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/ShishirPatil/gorilla)  [![arXiv](https://img.shields.io/badge/arXiv-2305.15334-<COLOR>.svg?style=flat-square)](https://arxiv.org/abs/2305.15334)   [![Discord](https://img.shields.io/discord/1111172801899012102?label=Discord&logo=discord&logoColor=green&style=flat-square)](https://discord.gg/SwTyuTAxX3)  [![Twitter](https://img.shields.io/twitter/url?url=https://twitter.com/shishirpatil_/status/1661780076277678082)](https://twitter.com/shishirpatil_/status/1661780076277678082)\n",
        "\n",
        "Play around with Gorilla! Here, we show how you can use OpenFunctions from Gorilla, so you can try it out! This is compatible with the OpenAI chat completion API - plug and play!\n",
        "\n",
        "üü¢ Now with Apache-2.0! Gorilla is commercially usable with no obligations üöÄ\n",
        "\n",
        "\n",
        "üíÉ If you want to use Gorilla or build on top of it! Feel absolutely free to do so - we believe in open source research and you don't even have to tell us! In case you choose to do, we have a vibrant community in Discord! Stop by and say Hi üëã\n",
        "\n",
        "<img src=\"https://github.com/ShishirPatil/gorilla/blob/gh-pages/assets/img/logo.png?raw=true\" width=30% height=30%>"
      ],
      "metadata": {
        "id": "7bKku43frr8M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gorilla ü¶ç is hosted by UC Berkeley Sky lab for FREE ü§© as a research prototype ü§ì Please don't use it for commercial serving üëÄ\n"
      ],
      "metadata": {
        "id": "5PA9GQbV4rcN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eBd_fso7qFPX"
      },
      "outputs": [],
      "source": [
        "# Import Chat completion template and set-up variables\n",
        "!pip install openai==0.28.1 &> /dev/null\n",
        "import openai\n",
        "import urllib.parse\n",
        "import json\n",
        "\n",
        "\n",
        "# Report issues\n",
        "def raise_issue(e, model, prompt):\n",
        "    issue_title = urllib.parse.quote(\"[bug] Hosted Gorilla: <Issue>\")\n",
        "    issue_body = urllib.parse.quote(f\"Exception: {e}\\nFailed model: {model}, for prompt: {prompt}\")\n",
        "    issue_url = f\"https://github.com/ShishirPatil/gorilla/issues/new?assignees=&labels=hosted-gorilla&projects=&template=hosted-gorilla-.md&title={issue_title}&body={issue_body}\"\n",
        "    print(f\"An exception has occurred: {e} \\nPlease raise an issue here: {issue_url}\")\n",
        "\n",
        "# Query Gorilla server\n",
        "def get_gorilla_response(prompt=\"Call me an Uber ride type \\\"Plus\\\" in Berkeley at zipcode 94704 in 10 minutes\", model=\"gorilla-openfunctions-v0\", functions=[]):\n",
        "  openai.api_key = \"EMPTY\" # Hosted for free with ‚ù§Ô∏è from UC Berkeley\n",
        "  openai.api_base = \"http://luigi.millennium.berkeley.edu:8000/v1\"\n",
        "  try:\n",
        "    completion = openai.ChatCompletion.create(\n",
        "      model=\"gorilla-openfunctions-v1\",\n",
        "      temperature=0.0,\n",
        "      messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "      functions=functions,\n",
        "    )\n",
        "    return completion.choices[0].message.content\n",
        "  except Exception as e:\n",
        "    print(e, model, prompt)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üöÄ Using gorilla is as easy as calling `get_gorilla_response()` with your prompt! Try out Gorilla, and share your interesting findings in `#showcase` ü§© [Discord](https://discord.gg/3apqwwME)!"
      ],
      "metadata": {
        "id": "XS5Qe6zD8tdX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gorilla OpenFunctions\n",
        "\n",
        "## Gorilla OpenFunctions is a drop in replacement for OpenAI Function Call API!\n",
        "\n",
        "## Introduction\n",
        "**OpenFunctions** is designed to extend Large Language Model (LLM) Chat Completion features by formulating executable API calls from natural language instructions and API context. With this LLMs can fill parameters for a diverse range of services, from Instagram and Doordash to tools like Google Calendar and Stripe, to enterprise services such as Salesforce and Datadog. With Open Functions, even those unfamiliar with API calls or programming can use the model to generate desired API calls. Trained on a curated collection of API documentations and associated Q&A pairs, OpenFunctions is another step in the Gorilla Paradigm's ongoing evolution, aiming for enhanced quality and accuracy in function call generation.\n",
        "\n",
        "<!-- Insert Image here -->\n",
        "\n",
        "## Code Function Calling API vs. REST API\n",
        "Throughout our data collection process, we've discerned that general API calling can broadly bifurcate into two categories:\n",
        "\n",
        "1. **Code Function Calling APIs**:\n",
        "    - Predominantly observed in external Python packages like Numpy and Sklearn.\n",
        "    - Characterized by well-defined and easily formatted calls.\n",
        "    - Simply knowing the `api_name` (e.g., `numpy.sum()`) and `arguments` specifications allows the extrapolation of an executable function API.\n",
        "    - Owing to its consistent format and fixed locality, fine-tuning the model requires relatively minimal data.\n",
        "\n",
        "2. **REST APIs**\n",
        "    - Traditional `GET` and `POST` requests.\n",
        "\n",
        "## How to use Open Functions\n",
        "Leveraging **Gorilla OpenFunctions** is refreshingly straightforward:\n",
        "\n",
        "1. **Define Your Functions**:\n",
        "    - Furnish a JSON file detailing your custom functions.\n",
        "    - Each function should encompass fields: `name`, `api_call`, `description`, and `parameters`.\n",
        "    - Below is an example for a comprehensive API documentation suitable for Open Function:\n",
        "      ```python\n",
        "      function_documentation = {  \n",
        "          \"name\" : \"Order Food on Uber\",\n",
        "          \"api_call\": \"uber.eat.order\",\n",
        "          \"description\": \"Order food on uber eat, specifying items and their quantities\",\n",
        "          \"parameters\": [\n",
        "              {\n",
        "                  \"name\": \"restaurants\",\n",
        "                  \"description\": \"The chosen restaurant\"\n",
        "              },\n",
        "              {\n",
        "                  \"name\": \"items\",\n",
        "                  \"description\": \"List of selected items\"\n",
        "              },\n",
        "              {\n",
        "                  \"name\": \"quantities\",\n",
        "                  \"description\": \"Quantities corresponding to the chosen items\"\n",
        "              }\n",
        "          ]\n",
        "      }\n",
        "      ```\n",
        "\n",
        "2. **Ask Your Question**:\n",
        "    - Frame your requirement conversationally.\n",
        "    - For instance: *I want to order five burgers and six chicken wings from McDonald's.*\n",
        "\n",
        "3. **Get Your Function Call**:\n",
        "    - The model deciphers your request and reciprocates with a Python function call.\n",
        "    - This paradigm expands horizons for both developers and laypersons, enabling them to harness intricate functionalities sans extensive coding.\n",
        "      ```python\n",
        "      Input:\n",
        "      get_gorilla_response(prompt=\"I want to order five burgers and six chicken wings from McDonald's.\", functions=[function_documentation])\n",
        "      \n",
        "      Output:\n",
        "      uber.eat.order(restaurants=\"McDonald\", items=[\"chicken wings\", \"burgers\"], quantities=[6,5])\n",
        "      ```\n",
        "\n"
      ],
      "metadata": {
        "id": "fCh9IGKkMxNy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Weather Example\n",
        "\n"
      ],
      "metadata": {
        "id": "-tv21HQwiG8K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import openai\n",
        "import json\n",
        "\n",
        "# Example dummy function hard coded to return the same weather\n",
        "# In production, this could be your backend API or an external API\n",
        "def get_current_weather(location, unit=\"fahrenheit\"):\n",
        "    \"\"\"Get the current weather in a given location\"\"\"\n",
        "    weather_info = {\n",
        "        \"location\": location,\n",
        "        \"temperature\": \"72\",\n",
        "        \"unit\": unit,\n",
        "        \"forecast\": [\"sunny\", \"windy\"],\n",
        "    }\n",
        "    return json.dumps(weather_info)\n",
        "\n",
        "def run_conversation():\n",
        "    # Step 1: send the conversation and available functions to GPT\n",
        "    messages = [{\"role\": \"user\", \"content\": \"What's the weather like in Boston?\"}]\n",
        "    functions = [\n",
        "        {\n",
        "            \"name\": \"get_current_weather\",\n",
        "            \"description\": \"Get the current weather in a given location\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"location\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
        "                    },\n",
        "                    \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
        "                },\n",
        "                \"required\": [\"location\"],\n",
        "            },\n",
        "        }\n",
        "    ]\n",
        "    openai.api_key = \"EMPTY\" # Hosted for free with ‚ù§Ô∏è from UC Berkeley\n",
        "    openai.api_base = \"http://luigi.millennium.berkeley.edu:8000/v1\"\n",
        "    response = openai.ChatCompletion.create(\n",
        "        # model=\"gpt-3.5-turbo-0613\",\n",
        "        model='gorilla-openfunctions-v0',\n",
        "        messages=messages,\n",
        "        functions=functions,\n",
        "        function_call=\"auto\",  # auto is default, but we'll be explicit\n",
        "    )\n",
        "    response_message = response[\"choices\"][0][\"message\"]\n",
        "    print(response_message)\n",
        "\n",
        "\n",
        "\n",
        "run_conversation()"
      ],
      "metadata": {
        "id": "RgCzhtFSlcUk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "062517c2-ec16-4bb9-a675-a1b8d59c57f2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"role\": \"assistant\",\n",
            "  \"content\": \"get_current_weather(location=\\\"Boston\\\", unit=\\\"celsius\\\")\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Here are some more examples. Just run the cells to see how Gorilla performs!\n",
        "\n",
        "You can edit any of the blocks in-place, and try it out yourself!"
      ],
      "metadata": {
        "id": "JVWCfdLZo9jH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Uber"
      ],
      "metadata": {
        "id": "xOuyyQ7NeJS3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Call me an Uber ride type \\\"Plus\\\" in Berkeley at zipcode 94704 in 10 minutes\"\n",
        "functions = [\n",
        "    {\n",
        "        \"name\": \"Uber Carpool\",\n",
        "        \"api_name\": \"uber.ride\",\n",
        "        \"description\": \"Find suitable ride for customers given the location, type of ride, and the amount of time the customer is willing to wait as parameters\",\n",
        "        \"parameters\":  [{\"name\": \"loc\", \"description\": \"location of the starting place of the uber ride\"}, {\"name\":\"type\", \"enum\": [\"plus\", \"comfort\", \"black\"], \"description\": \"types of uber ride user is ordering\"}, {\"name\": \"time\", \"description\": \"the amount of time in minutes the customer is willing to wait\"}]\n",
        "    }\n",
        "]\n",
        "get_gorilla_response(query, functions=functions)"
      ],
      "metadata": {
        "id": "5DrGQWd4CKIj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0f4be800-f781-4346-9f36-592f25fd9451"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'uber.ride(loc=\"94704\", type=\"plus\", time=10)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "function_documentation = [{\n",
        "    \"name\" : \"Order Food on Uber\",\n",
        "    \"api_name\": \"uber.eat.order\",\n",
        "    \"description\": \"Order food on uber eat, specifying items and their quantities\",\n",
        "    \"parameters\": [\n",
        "        {\n",
        "            \"name\": \"restaurants\",\n",
        "            \"description\": \"The chosen restaurant\"\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"items\",\n",
        "            \"description\": \"List of selected items\"\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"quantities\",\n",
        "            \"description\": \"Quantities corresponding to the chosen items\"\n",
        "        }\n",
        "    ]\n",
        "}]\n",
        "query =  \"I want to order five 'burgers' and six 'chicken wings' from uber eat McDonald's.\"\n",
        "get_gorilla_response(query, functions=function_documentation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "PBqcermvk56c",
        "outputId": "528527ab-61b8-4a53-f78b-d9515c7b1f40"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'uber.eat.order(restaurants=\"McDonald\\'s\", items=[\"burgers\", \"chicken wings\"], quantities=[5, 6])'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AWS"
      ],
      "metadata": {
        "id": "HZhyO8RdeLff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from os.path import join\n",
        "query = \"I want to list the exports for my bot with the bot id \\\"my-bot-id\\\" and the bot version \\\"v2\\\".\"\n",
        "functions = [\n",
        "    {\n",
        "        \"domain\": \"Cloud Infrastructure\",\n",
        "        \"framework\": \"aws\",\n",
        "        \"functionality\": \"Lists the exports for a bot, bot locale, or custom vocabulary. Exports are kept in the list for 7 days.\",\n",
        "        \"api_name\": \"aws.lexv2-models.list-exports\",\n",
        "        \"api_arguments\": [\n",
        "            {\n",
        "                \"name\": \"bot-id\",\n",
        "                \"description\": \"\\nThe unique identifier that Amazon Lex assigned to the bot.\"\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"bot-version\",\n",
        "                \"description\": \"\\nThe version of the bot to list exports for.\"\n",
        "            }\n",
        "        ],\n",
        "        \"python_environment_requirements\": [\n",
        "            \"aws\"\n",
        "        ],\n",
        "        \"example_code\": [],\n",
        "        \"output\": {\n",
        "            \"botId -> (string)\": \"\\nThe unique identifier assigned to the bot by Amazon Lex.\",\n",
        "            \"botVersion -> (string)\": \"\\nThe version of the bot that was exported.\",\n",
        "            \"exportSummaries -> (list)\": \"\\nSummary information for the exports that meet the filter criteria specified in the request. The length of the list is specified in the maxResults parameter. If there are more exports available, the nextToken field contains a token to get the next page of results.\\n(structure)\\n\\nProvides summary information about an export in an export list.\\nexportId -> (string)\\n\\nThe unique identifier that Amazon Lex assigned to the export.\\nresourceSpecification -> (structure)\\n\\nInformation about the bot or bot locale that was exported.\\nbotExportSpecification -> (structure)\\n\\nParameters for exporting a bot.\\nbotId -> (string)\\n\\nThe identifier of the bot assigned by Amazon Lex.\\nbotVersion -> (string)\\n\\nThe version of the bot that was exported. This will be either DRAFT or the version number.\\n\\nbotLocaleExportSpecification -> (structure)\\n\\nParameters for exporting a bot locale.\\nbotId -> (string)\\n\\nThe identifier of the bot to create the locale for.\\nbotVersion -> (string)\\n\\nThe version of the bot to export.\\nlocaleId -> (string)\\n\\nThe identifier of the language and locale to export. The string must match one of the locales in the bot.\\n\\ncustomVocabularyExportSpecification -> (structure)\\n\\nThe parameters required to export a custom vocabulary.\\nbotId -> (string)\\n\\nThe identifier of the bot that contains the custom vocabulary to export.\\nbotVersion -> (string)\\n\\nThe version of the bot that contains the custom vocabulary to export.\\nlocaleId -> (string)\\n\\nThe locale of the bot that contains the custom vocabulary to export.\\n\\ntestSetExportSpecification -> (structure)\\n\\nSpecifications for the test set that is exported as a resource.\\ntestSetId -> (string)\\n\\nThe unique identifier of the test set.\\n\\n\\nfileFormat -> (string)\\n\\nThe file format used in the export files.\\nexportStatus -> (string)\\n\\nThe status of the export. When the status is Completed the export is ready to download.\\ncreationDateTime -> (timestamp)\\n\\nThe date and time that the export was created.\\nlastUpdatedDateTime -> (timestamp)\\n\\nThe date and time that the export was last updated.\\n\\n\",\n",
        "            \"nextToken -> (string)\": \"\\nA token that indicates whether there are more results to return in a response to the ListExports operation. If the nextToken field is present, you send the contents as the nextToken parameter of a ListExports operation request to get the next page of results.\",\n",
        "            \"localeId -> (string)\": \"\\nThe locale specified in the request.\"\n",
        "        },\n",
        "        \"api_arguments_all\": {\n",
        "            \"--bot-id \": \"\\nThe unique identifier that Amazon Lex assigned to the bot.\",\n",
        "            \"--bot-version \": \"\\nThe version of the bot to list exports for.\",\n",
        "            \"--sort-by \": \"\\nDetermines the field that the list of exports is sorted by. You can sort by the LastUpdatedDateTime field in ascending or descending order.\\nattribute -> (string)\\n\\nThe export field to use for sorting.\\norder -> (string)\\n\\nThe order to sort the list.\\n\",\n",
        "            \"--filters \": \"\\nProvides the specification of a filter used to limit the exports in the response to only those that match the filter specification. You can only specify one filter and one string to filter on.\\n(structure)\\n\\nFilters the response form the ListExports operation\\nname -> (string)\\n\\nThe name of the field to use for filtering.\\nvalues -> (list)\\n\\nThe values to use to filter the response. The values must be Bot , BotLocale , or CustomVocabulary .\\n(string)\\n\\noperator -> (string)\\n\\nThe operator to use for the filter. Specify EQ when the ListExports operation should return only resource types that equal the specified value. Specify CO when the ListExports operation should return resource types that contain the specified value.\\n\\n\",\n",
        "            \"--max-results \": \"\\nThe maximum number of exports to return in each page of results. If there are fewer results than the max page size, only the actual number of results are returned.\",\n",
        "            \"--next-token \": \"\\nIf the response from the ListExports operation contains more results that specified in the maxResults parameter, a token is returned in the response.\\nUse the returned token in the nextToken parameter of a ListExports request to return the next page of results. For a complete set of results, call the ListExports operation until the nextToken returned in the response is null.\\n\",\n",
        "            \"--locale-id \": \"\\nSpecifies the resources that should be exported. If you don\\u00e2\\u0080\\u0099t specify a resource type in the filters parameter, both bot locales and custom vocabularies are exported.\"\n",
        "        }\n",
        "    }\n",
        "]\n",
        "get_gorilla_response(query, functions=functions)"
      ],
      "metadata": {
        "id": "CYuQm0Dm_8Np",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ac84a0f3-2602-4ab7-8c9f-8c220603c062"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'aws.lexv2-models.list-exports(bot-id=\"my-bot-id\", bot-version=\"v2\")'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# More on Function Calling from OpenAI\n",
        "\n",
        "Function calling allows you to more reliably get structured data back from the model. For example, you can:\n",
        "\n",
        "## Use Cases\n",
        "\n",
        "- **Chatbots with API calls**: Create chatbots that answer questions by calling external APIs (e.g. like ChatGPT Plugins)\n",
        "  - e.g. `send_email(to: string, body: string)`\n",
        "  - e.g. `get_current_weather(location: string, unit: 'celsius' | 'fahrenheit')`\n",
        "  \n",
        "- **Natural Language to API Conversion**: Convert natural language into API calls\n",
        "  - e.g. Convert \"Who are my top customers?\" to `get_customers(min_revenue: int, created_before: string, limit: int)` and call your internal API\n",
        "  \n",
        "- **Data Extraction**: Extract structured data from text\n",
        "  - e.g. `extract_data(name: string, birthday: string)`\n",
        "  - e.g. `sql_query(query: string)`\n",
        "\n",
        "## Workflow\n",
        "\n",
        "The basic sequence of steps for function calling is as follows:\n",
        "\n",
        "1. **User Query**: Call the model with the user query and a set of functions defined in the `functions` parameter.\n",
        "2. **Function Invocation**: The model can choose to call a function; if so, the content will be a stringified JSON object adhering to your custom schema (note: the model may generate invalid JSON or hallucinate parameters).\n",
        "3. **Parse and Execute**: Parse the string into JSON in your code, and call your function with the provided arguments if they exist.\n",
        "4. **Response**: Call the model again by appending the function response as a new message, and let the model summarize the results back to the user.\n",
        "\n",
        "> **Tip**: You can see these steps in action through the example below:\n"
      ],
      "metadata": {
        "id": "utIgeFTzpjdN"
      }
    }
  ]
}